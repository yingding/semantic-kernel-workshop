{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Semantic Kernel Introduction\n",
    "\n",
    "### Overview of Semantic Kernel (SK) and Its Importance\n",
    "\n",
    "**Semantic Kernel** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is *lightweight* and *modular*, designed for **enterprise-grade solutions** with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and **future-proof** – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build **robust, scalable AI applications** that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- **Bridging AI and Code**: SK combines natural language **prompts** with your **existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI *intends* and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK **plugins**. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions **modular and extensible**.\n",
    "- **Enterprise-ready**: SK includes support for **security, observability, and compliance** (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining *what* you want the AI to do (skills, prompts) and SK handles *how* to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to **automatically call functions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Services and Core Components of SK\n",
    "\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The `Kernel` holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "  - *Chat Models*: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "  - *Embedding Models*: for converting text to vector embeddings (used in memory/search).\n",
    "  - *Other Modalities*: connectors for images, speech, etc., if needed.\n",
    "  \n",
    "  You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantic-kernel python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import (\n",
    "    AzureChatCompletion,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "    endpoint=base_url,\n",
    "    deployment_name=deployment_name,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully created a kernel and added a chat completion service.\n",
    "\n",
    "Similarly, you can add an embedding generation service via `kernel.add_service(text_embedding)` if performing semantic memory searches. But don't worry we will dive into this at a later stage.\n",
    "\n",
    "---\n",
    "\n",
    "### Functions and Plugins in SK\n",
    "\n",
    "**Functions** in Semantic Kernel are the actions that the AI can perform. They come in two types:\n",
    "\n",
    "- **Semantic Functions**: Backed by a prompt and LLM. For example, a function `TranslateToFrench` might use the prompt `\"Translate this to French: {{$input}}\"`.\n",
    "- **Native Functions**: Backed by code. For example, a function `SendEmail(to, subject, body)` that uses an API to send an email.\n",
    "\n",
    "These functions are typically grouped into **Plugins** (or \"Skills\"). Grouping functions into plugins helps manage and control which capabilities are exposed to the AI.\n",
    "\n",
    "**Using Plugins/Functions**: Once registered with the kernel (via `kernel.add_function` or `kernel.add_plugin`), functions become available for invocation. They can be called directly in code via `kernel.invoke(function, input)`, or the AI model can automatically choose to invoke them as needed.\n",
    "\n",
    "SK’s plugin system is highly flexible:\n",
    "- You can load **OpenAPI** specifications or API endpoints as plugins.\n",
    "- Plugins can be shared across projects, allowing organizations to build a library of useful AI plugins.\n",
    "\n",
    "---\n",
    "\n",
    "You can add plugins to the Kernel in various ways:\n",
    "\n",
    "- **Inline Definition**: Define a prompt or function in code and register it.\n",
    "- **From Files or Classes**: Load plugins from directories or Python classes decorated appropriately.\n",
    "\n",
    "For example, to add a simple **semantic function** inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a semantic function (prompt) to generate a TL;DR summary\n",
    "prompt_template = \"{{$input}}\\n\\nTL;DR in one sentence:\"\n",
    "\n",
    "summarize_fn = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"tldr\",\n",
    "    plugin_name=\"Summarizer\",\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "long_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(summarize_fn, input=long_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a translation function\n",
    "\n",
    "Write a semantic function that translates text to a target language.\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "  # Define a semantic function (prompt) to generate a translation\n",
    "prompt_template = \"{{$input}}\\n\\nTranslate this into {{$target_lang}}:\"\n",
    "\n",
    "translate_fn = kernel.add_function(\n",
    "    prompt=prompt_template, \n",
    "    function_name=\"translator\", \n",
    "    plugin_name=\"Translator\",\n",
    "    max_tokens=50)\n",
    "\n",
    "# Use the function\n",
    "text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(translate_fn, input=text, target_lang=\"French\")\n",
    "print(summary)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the previous implementation for reference, if you are stuck view the solution.\n",
    "\n",
    "# Your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now if we want to build a Plugin with a set of **native functions** we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependencies for the plugin\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "\n",
    "\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"Please turn on the lamp\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Function Calling\n",
    "\n",
    "One of the most powerful features of Semantic Kernel is its ability to **automatically orchestrate multi-step operations** by calling multiple functions in sequence. In this example, a **LightsPlugin** is defined with three asynchronous native functions:\n",
    "\n",
    "- **get_lights()**: Retrieves the list of lights along with their current states.\n",
    "- **get_state(id)**: Returns the state of a specific light, given its ID.\n",
    "- **change_state(id, new_state)**: Changes the state of a specified light to a new state (for example, turning it on, adjusting brightness, or changing its color).\n",
    "\n",
    "**How It Works**:\n",
    "- The kernel exposes all registered functions to the AI model.\n",
    "- When a user issues a command like *\"Turn on all the lights and give me their final state,\"* the AI model analyzes the request and plans the necessary steps:\n",
    "  1. Call **get_lights()** to retrieve all available lights.\n",
    "  2. For each light, invoke **change_state()** with a new state (e.g., setting `\"is_on\": True`).\n",
    "  3. Optionally, call **get_state()** for each light to confirm the updated status.\n",
    "- The kernel then returns a comprehensive result that reflects the final state of each light.\n",
    "\n",
    "**Example Scenario**:\n",
    "- **User Query**: *\"Turn on all the lights and tell me their status.\"*\n",
    "- **Step 1**: The system calls `LightsPlugin.get_lights()` to fetch the current list of lights.\n",
    "- **Step 2**: It iterates over the list and calls `LightsPlugin.change_state(id, new_state)` to turn each light on.\n",
    "- **Step 3**: Finally, it may call `LightsPlugin.get_state(id)` for each light to confirm the changes.\n",
    "- The final output, including the updated state for each light, is returned to the user.\n",
    "\n",
    "This automatic orchestration simplifies the management of multi-step tasks, enabling the AI to autonomously plan and execute function calls without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try other messages and observe the results\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Exercise: Creating a Weather Information Plugin\n",
    "\n",
    "In this exercise, you'll create a native plugin for Semantic Kernel that simulates a weather service. This will help you understand how to create and use native functions in a plugin.\n",
    "\n",
    "### Task:\n",
    "1. Create a `WeatherPlugin` class with the following functions:\n",
    "   - `get_current_weather(location)`: Returns the current weather for a given location\n",
    "   - `get_forecast(location, days)`: Returns a weather forecast for a specified number of days\n",
    "   - `get_weather_alert(location)`: Returns any active weather alerts for a location\n",
    "\n",
    "2. Register the plugin with the kernel and test it with a few user queries\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "    from typing import Annotated, List, Dict\n",
    "    from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "    import random\n",
    "\n",
    "    class WeatherPlugin:\n",
    "        def __init__(self):\n",
    "            # Simulated weather data\n",
    "            self.weather_conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\", \"Windy\", \"Foggy\", \"Stormy\"]\n",
    "            self.temperature_ranges = {\n",
    "                \"New York\": (50, 85),\n",
    "                \"London\": (45, 75),\n",
    "                \"Tokyo\": (55, 90),\n",
    "                \"Sydney\": (60, 95),\n",
    "                \"Paris\": (48, 80),\n",
    "                \"Default\": (40, 100)\n",
    "            }\n",
    "            \n",
    "            # Simulated alerts\n",
    "            self.alerts = {\n",
    "                \"New York\": \"Heat advisory in effect\",\n",
    "                \"Tokyo\": \"Typhoon warning for coastal areas\",\n",
    "                \"Sydney\": None,\n",
    "                \"London\": None,\n",
    "                \"Paris\": \"Air quality warning\"\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_current_weather(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get weather for\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets the current weather for a specified location.\"\"\"\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            temperature = random.randint(temp_range[0], temp_range[1])\n",
    "            condition = random.choice(self.weather_conditions)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"temperature\": temperature,\n",
    "                \"condition\": condition,\n",
    "                \"humidity\": random.randint(30, 95),\n",
    "                \"wind_speed\": random.randint(0, 30)\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_forecast(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get forecast for\"],\n",
    "            days: Annotated[int, \"Number of days for the forecast\"] = 3\n",
    "        ) -> List[Dict]:\n",
    "            \"\"\"Gets a weather forecast for a specified number of days.\"\"\"\n",
    "            forecast = []\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            \n",
    "            for i in range(days):\n",
    "                forecast.append({\n",
    "                    \"day\": i + 1,\n",
    "                    \"temperature\": random.randint(temp_range[0], temp_range[1]),\n",
    "                    \"condition\": random.choice(self.weather_conditions),\n",
    "                    \"humidity\": random.randint(30, 95),\n",
    "                    \"wind_speed\": random.randint(0, 30)\n",
    "                })\n",
    "            \n",
    "            return forecast\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_weather_alert(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to check for weather alerts\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets any active weather alerts for a location.\"\"\"\n",
    "            alert = self.alerts.get(location)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"has_alert\": alert is not None,\n",
    "                \"alert_message\": alert if alert else \"No active alerts\"\n",
    "            }\n",
    "\n",
    "    # Usage example:\n",
    "    # Create the plugin\n",
    "    weather_plugin = WeatherPlugin()\n",
    "\n",
    "    # Register with kernel\n",
    "    kernel.add_plugin(\n",
    "        plugin=weather_plugin,\n",
    "        plugin_name=\"Weather\"\n",
    "    )\n",
    "\n",
    "    # Test with a user query\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What's the weather like in Tokyo and are there any alerts?\")\n",
    "\n",
    "    # Get response using function calling\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(\"Assistant > \" + str(result))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### Learning Objectives:\n",
    "- Creating a native plugin with multiple functions\n",
    "- Using type annotations for function parameters\n",
    "- Registering a plugin with the kernel\n",
    "- Testing the plugin with natural language queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Exercise\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters in Semantic Kernel\n",
    "\n",
    "Filters provide a layer of control and visibility over function execution, ensuring responsible AI practices and enterprise-grade security. They allow you to:\n",
    "\n",
    "- **Validate Permissions:**  \n",
    "  For example, a filter can check user permissions before initiating an approval flow.\n",
    "\n",
    "- **Intercept Function Execution:**  \n",
    "  - **Function Invocation Filter:**  \n",
    "    Runs every time a function is called; it can access function details, handle exceptions, override results (e.g., for caching or responsible AI), or retry on failure.\n",
    "  - **Prompt Render Filter:**  \n",
    "    Triggered before a prompt is rendered; it allows you to view or modify the prompt and even override the result to prevent submission.\n",
    "  - **Auto Function Invocation Filter:**  \n",
    "    Works within automatic function calling, providing additional context (like chat history and iteration counters) and can terminate the process early if needed.\n",
    "\n",
    "Each filter receives a context object with execution details and must call the next delegate (or callback) to continue the execution chain. Filters can be registered either by using the `add_filter` method on the Kernel or via the `@kernel.filter` decorator.\n",
    "\n",
    "\n",
    "One of the things that I would like to improve in our plugin implementation is to add debugging, that way I can integrate with external systems for auditing purposes\n",
    "\n",
    "Lets implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Awaitable, Callable\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "async def logger_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    logger.info(\n",
    "        f\"FunctionInvoking - {context.function.plugin_name}.{context.function.name}\"\n",
    "    )\n",
    "\n",
    "    await next(context)\n",
    "\n",
    "    logger.info(\n",
    "        f\"FunctionInvoked - {context.function.plugin_name}.{context.function.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "kernel.add_filter(\"function_invocation\", logger_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message(\"Please turn on all the lamps and give me their final state\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like this\n",
    "\n",
    "```shell\n",
    "2025-02-27 15:21:00,969 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:00,971 - INFO - OpenAI usage: CompletionUsage(completion_tokens=84, prompt_tokens=407, total_tokens=491, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "2025-02-27 15:21:00,971 - INFO - processing 2 tool calls in parallel.\n",
    "2025-02-27 15:21:00,972 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,972 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,973 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,973 - INFO - Function completed. Duration: 0.000904s\n",
    "2025-02-27 15:21:00,974 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,975 - INFO - Function completed. Duration: 0.000567s\n",
    "2025-02-27 15:21:06,333 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:06,335 - INFO - OpenAI usage: CompletionUsage(completion_tokens=120, prompt_tokens=572, total_tokens=692, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "Assistant > All the lights are now turned on. Here's the final state of each lamp:\n",
    "\n",
    "1. **Table Lamp**\n",
    "   - Status: On\n",
    "   - Brightness: 100%\n",
    "   - Color: Red (#FF0000)\n",
    "\n",
    "2. **Porch Light**\n",
    "   - Status: On\n",
    "   - Brightness: 50%\n",
    "   - Color: Green (#00FF00)\n",
    "\n",
    "3. **Chandelier**\n",
    "   - Status: On\n",
    "   - Brightness: 75%\n",
    "   - Color: Blue (#0000FF)\n",
    "\n",
    "If you need any further adjustments, just let me know!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Creating a Content Filter\n",
    "\n",
    "In this exercise, you'll create a filter that monitors and potentially modifies content flowing through the Semantic Kernel pipeline. This will help you understand how to implement safety and compliance measures in AI applications.\n",
    "\n",
    "### Task:\n",
    "1. Create a content filter that detects and redacts sensitive information (like credit card numbers, emails, etc.)\n",
    "2. Implement the filter as a pre-processing step for user inputs and a post-processing step for AI outputs\n",
    "3. Test the filter with various inputs containing sensitive information\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "import re\n",
    "from typing import Any, Coroutine\n",
    "from collections.abc import Callable as ABCCallable\n",
    "import logging\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "from semantic_kernel.functions import FunctionResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regular expressions for sensitive data patterns\n",
    "PATTERNS = {\n",
    "    'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',  # Credit card format: XXXX-XXXX-XXXX-XXXX\n",
    "    'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "    'phone': r'\\b(?:\\+\\d{1,3}[-\\s]?)?\\(?\\d{3}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\\b',  # Phone numbers\n",
    "    'ssn': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',  # Social Security Numbers (US)\n",
    "}\n",
    "\n",
    "class ContentFilter:\n",
    "    def __init__(self, patterns=PATTERNS):\n",
    "        self.patterns = patterns\n",
    "        \n",
    "    def redact_sensitive_info(self, text):\n",
    "        \"\"\"Redact sensitive information from text.\"\"\"\n",
    "        result = text\n",
    "        detected = []\n",
    "        \n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            matches = re.finditer(pattern, result)\n",
    "            for match in matches:\n",
    "                detected.append(f\"{pattern_name}: {match.group()}\")\n",
    "                result = result.replace(match.group(), f\"[REDACTED {pattern_name.upper()}]\")\n",
    "        \n",
    "        return result, detected\n",
    "\n",
    "# Create a pre-processing filter for user inputs\n",
    "async def input_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's an input parameter\n",
    "    if 'input' in context.arguments:\n",
    "        original_input = context.arguments['input']\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_input, detected = content_filter.redact_sensitive_info(original_input)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in input: {', '.join(detected)}\")\n",
    "            \n",
    "        # Replace the original input with the filtered version\n",
    "        context.arguments['input'] = filtered_input\n",
    "    \n",
    "    # Continue to the next filter or function\n",
    "    await next(context)\n",
    "\n",
    "# Create a post-processing filter for AI outputs\n",
    "async def output_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    # Continue to the next filter or function first\n",
    "    await next(context)\n",
    "    \n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's a result to filter\n",
    "    if context.result:\n",
    "        original_output = str(context.result)\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_output, detected = content_filter.redact_sensitive_info(original_output)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in output: {', '.join(detected)}\")\n",
    "         \n",
    "        # Create a new FunctionResult with the filtered output\n",
    "        context.result = FunctionResult(\n",
    "            function=context.function.metadata,\n",
    "            value=filtered_output,\n",
    "            metadata=context.result.metadata if hasattr(context.result, 'metadata') else {}\n",
    "        )\n",
    "\n",
    "async def test_content_filters():\n",
    "    \n",
    "    # Register the filters with the kernel\n",
    "    kernel.add_filter(\"function_invocation\", input_filter)\n",
    "    kernel.add_filter(\"function_invocation\", output_filter)\n",
    "    \n",
    "    # Create a simple semantic function\n",
    "    echo_prompt = \"{{$input}}\"\n",
    "    echo_fn = kernel.add_function(\n",
    "        prompt=echo_prompt,\n",
    "        function_name=\"echo\",\n",
    "        plugin_name=\"TestPlugin\"\n",
    "    )\n",
    "    \n",
    "    # Test with sensitive information\n",
    "    test_inputs = [\n",
    "        \"My credit card number is 4111-1111-1111-1111\",\n",
    "        \"Contact me at john.doe@example.com or call 555-123-4567\",\n",
    "        \"My SSN is 123-45-6789\",\n",
    "        \"Here's my info: john.doe@example.com, 4111-1111-1111-1111, 555-123-4567\",\n",
    "        \"What are the services that you can offer?\"\n",
    "    ]\n",
    "    \n",
    "    for input_text in test_inputs:\n",
    "        print(f\"\\nOriginal Input: {input_text}\")\n",
    "        result = await kernel.invoke(echo_fn, input=input_text)\n",
    "        print(f\"Filtered Output: {result}\")\n",
    "\n",
    "await test_content_filters()\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Objectives:\n",
    "- Creating and registering filters in Semantic Kernel\n",
    "- Implementing pre-processing and post-processing logic\n",
    "- Using regular expressions for pattern matching\n",
    "- Understanding the filter execution pipeline\n",
    "- Logging and monitoring sensitive information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Filters in SK and Their Use Cases\n",
    "\n",
    "To summarize, in any AI application, it’s important to control input/output and function execution for security, privacy, and correctness. **Filters** in Semantic Kernel act as middleware or interceptors in the execution pipeline.\n",
    "\n",
    "**Use Cases for Filters**:\n",
    "- **Security/Policy**: Prevent sensitive data from being sent to the AI.\n",
    "- **Validation**: Check function arguments before execution.\n",
    "- **Error Handling**: Catch exceptions and provide default results.\n",
    "- **Logging/Monitoring**: Log each function call and its response.\n",
    "- **Post-processing**: Modify outputs before they’re returned to the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
