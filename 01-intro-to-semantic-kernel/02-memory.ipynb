{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Memory in Semantic Kernel\n",
    "\n",
    "In AI applications, memory is crucial for creating contextual, personalized experiences. Semantic Kernel provides powerful memory management capabilities that allow your AI applications to:\n",
    "\n",
    "- Remember facts and knowledge over time\n",
    "- Find information based on meaning rather than exact matches\n",
    "- Use previous context in ongoing conversations\n",
    "- Implement Retrieval-Augmented Generation (RAG) patterns\n",
    "\n",
    "\n",
    "\n",
    "This notebook explores how to implement and use memory capabilities in Semantic Kernel applications. \n",
    "\n",
    "\n",
    "Let's visualize how memory fits into the Semantic Kernel architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"semantic-kernel[azure]==1.27.2\" python-dotenv==1.0.1 mermaid-py==0.7.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mermaid as md\n",
    "from mermaid.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Graph(\n",
    "    \"Sequence-diagram\",\n",
    "    \"\"\"\n",
    "graph TD\n",
    "    A[Application] --> B[Kernel]\n",
    "    B --> C[AI Models]\n",
    "    B --> D[Memory System]\n",
    "    B --> E[Plugins]\n",
    "    D --> F[Short-term Memory]\n",
    "    D --> G[Long-term Memory]\n",
    "    G --> H[Vector Embeddings]\n",
    "    G --> I[Memory Store]\n",
    "    I --> J[Volatile Store]\n",
    "    I --> K[Persistent Store]\n",
    "    style D fill:#f9d5e5,stroke:#333,stroke-width:2px\n",
    "\"\"\",\n",
    ")\n",
    "md.Mermaid(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Memory\n",
    "In SK, **memory** refers to the storage and recall of information the AI has learned or been provided. There are two primary forms of memory:\n",
    "\n",
    "### Semantic Memory (Long-term)\n",
    "- This is usually an **external vector store** that holds **embeddings of text**, allowing the AI to store facts or documents and later retrieve them by **semantic similarity**.\n",
    "- SK provides **Memory Connectors** to various vector databases (like **Azure Cognitive Search, Pinecone, Qdrant**, etc.) via a common interface.\n",
    "- By using a memory store, you can implement the **retrieval** part of **RAG**: store chunks of knowledge and fetch relevant pieces at query time.\n",
    "- Weâ€™ll see how to add and use such memory in our chatbot.\n",
    "\n",
    "### Conversation History (Short-term Memory)\n",
    "- SK also manages the **immediate dialogue context** with a **Chat History object** for multi-turn conversations.\n",
    "- This ensures the AI remembers prior user queries and its own responses, maintaining context across turns.\n",
    "- We will leverage this to keep the conversation coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import __version__\n",
    "\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import (\n",
    "    AzureChatCompletion,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import (\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_deployment_name = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-ada-002\"\n",
    ")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Create the embedding service\n",
    "embedding_service = AzureTextEmbedding(\n",
    "    endpoint=base_url, deployment_name=embedding_deployment_name, api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates an embedding service that connects to Azure OpenAI. This service will convert text into vector embeddings which are numerical representations that capture semantic meaning. The environment variables should be set in your `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(\n",
    "    storage=VolatileMemoryStore(), embeddings_generator=embedding_service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize our semantic memory system with:\n",
    "\n",
    "- A VolatileMemoryStore - an in-memory vector database (data will be lost when your session ends)\n",
    "- The embedding service we created earlier, which will generate vector embeddings for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"generic\"\n",
    "\n",
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await memory.save_information(\n",
    "        collection=collection_id, id=\"info1\", text=\"Your budget for 2024 is $100,000\"\n",
    "    )\n",
    "    await memory.save_information(\n",
    "        collection=collection_id, id=\"info2\", text=\"Your savings from 2023 are $50,000\"\n",
    "    )\n",
    "    await memory.save_information(\n",
    "        collection=collection_id, id=\"info3\", text=\"Your investments are $80,000\"\n",
    "    )\n",
    "\n",
    "\n",
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function adds information to our memory store. Each memory item consists of:\n",
    "\n",
    "- `collection`: A namespace for organizing related memories (like a database table)\n",
    "- `id`: A unique identifier for retrieving specific memories\n",
    "- `text`: The actual information to store\n",
    "\n",
    "\n",
    "When we save information, Semantic Memory:\n",
    "\n",
    "1. Generates an embedding vector for the text\n",
    "2. Stores both the text and its vector in the memory store\n",
    "3. Associates it with the given ID and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(memory: SemanticTextMemory) -> None:\n",
    "    questions = [\n",
    "        \"What is my budget for 2024?\",\n",
    "        \"What are my savings from 2023?\",\n",
    "        \"What are my investments?\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        result = await memory.search(collection_id, question)\n",
    "        print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await search_memory_examples(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How does semantic search work?\n",
    "\n",
    "1. We provide a natural language query (e.g., \"What is my budget for 2024?\")\n",
    "2. The memory system:\n",
    "   - Converts the query to a vector embedding\n",
    "   - Compares this vector against stored embeddings using cosine similarity\n",
    "   - Returns the closest matching results\n",
    "   \n",
    "The search works even if the query doesn't exactly match the stored text, as it finds semantically similar content.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Adding and Retrieving Custom Memories\n",
    "\n",
    "Try adding your own information to the memory and retrieving it with semantic search.\n",
    "\n",
    "1. Create a new collection called \"personal\"\n",
    "2. Add at least three facts about a fictional person\n",
    "3. Search for those facts using natural language queries\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "```python\n",
    "# Create a new collection\n",
    "personal_collection = \"personal\"\n",
    "\n",
    "# Add information to memory\n",
    "async def add_personal_info(memory):\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact1\", text=\"John was born in Seattle in 1980\")\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact2\", text=\"John graduated from University of Washington in 2002\")\n",
    "    await memory.save_information(collection=personal_collection, id=\"fact3\", text=\"John has two children named Alex and Sam\")\n",
    "\n",
    "await add_personal_info(memory)\n",
    "\n",
    "# Search for information\n",
    "questions = [\n",
    "    \"Where was John born?\",\n",
    "    \"When did John graduate college?\",\n",
    "    \"Does John have kids?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    result = await memory.search(personal_collection, question)\n",
    "    print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# Create a new collection\n",
    "\n",
    "# Add information to memory\n",
    "\n",
    "# Search for information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Setup\n",
    "\n",
    "This code creates a new Kernel instance and adds both:\n",
    "\n",
    "1. A chat completion service for generating responses\n",
    "2. The embedding service we created earlier for vector operations\n",
    "\n",
    "This configuration allows the kernel to generate text and work with vector embeddings in memory operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import (\n",
    "    AzureChatCompletion,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "    endpoint=base_url,\n",
    "    deployment_name=deployment_name,\n",
    "    api_key=api_key,\n",
    "    service_id=\"chat\",\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# we also add the embedding service to the kernel\n",
    "kernel.add_service(embedding_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(\n",
    "    storage=VolatileMemoryStore(), embeddings_generator=embedding_service\n",
    ")\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we:\n",
    "1. Create a `SemanticTextMemory` object with our in-memory store and embedding service\n",
    "2. Add the `TextMemoryPlugin` to the kernel, which provides memory-related functions\n",
    "\n",
    "The `TextMemoryPlugin` exposes memory operations to the kernel, allowing:\n",
    "- Semantic search through the `recall` function\n",
    "- Saving new information during conversations\n",
    "- Integration of memory capabilities into AI responses\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Now we set up a chat function that incorporates memory:\n",
    "\n",
    "1. We define a prompt template that:\n",
    "   - Gives the AI a role and instructions\n",
    "   - Uses the `{{recall '...'}}` syntax to search memory for relevant information\n",
    "   - Includes the user's request via `{{$request}}`\n",
    "\n",
    "2. We create a kernel function from this template\n",
    "\n",
    "The `{{recall 'query'}}` syntax tells Semantic Kernel to:\n",
    "1. Search the memory for information relevant to the query\n",
    "2. Insert the retrieved information into the prompt\n",
    "3. Let the AI use this information in its response\n",
    "\n",
    "This creates a chatbot that can reference previously stored financial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelFunction\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "\n",
    "    # - {{recall 'budget by year'}} What is my budget for 2024?\n",
    "    # - {{recall 'savings from previous year'}} What are my savings from 2023?\n",
    "    # - {{recall 'investments'}} What are my investments?\n",
    "async def setup_chat_with_memory(\n",
    "    kernel: Kernel,\n",
    "    service_id: str,\n",
    ") -> KernelFunction:\n",
    "    prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{recall 'budget by year'}}\n",
    "    - {{recall 'savings from previous year'}}\n",
    "    - {{recall 'investments'}}\n",
    "\n",
    "    {{$request}}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    prompt_template_config = PromptTemplateConfig(\n",
    "        template=prompt,\n",
    "        execution_settings={\n",
    "            service_id: kernel.get_service(\n",
    "                service_id\n",
    "            ).get_prompt_execution_settings_class()(service_id=service_id)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return kernel.add_function(\n",
    "        function_name=\"chat_with_memory\",\n",
    "        plugin_name=\"chat\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Populating memory...\")\n",
    "await populate_memory(memory)\n",
    "\n",
    "print(\"Asking questions... (manually)\")\n",
    "await search_memory_examples(memory)\n",
    "\n",
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func = await setup_chat_with_memory(kernel, \"chat\")\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try asking a question about your finances (i.e. \\\"talk to me about my finances\\\").\"\n",
    ")\n",
    "\n",
    "\n",
    "async def chat(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "    answer = await kernel.invoke(chat_func, request=user_input)\n",
    "    print(f\"ChatBot:> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"What is my budget for 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"talk to me about my finances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# we also add the embedding service to the kernel\n",
    "kernel.add_service(embedding_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) with Self-Critique\n",
    "\n",
    "This section demonstrates a powerful pattern combining memory retrieval with response evaluation:\n",
    "\n",
    "1. **RAG Prompt**: This prompt template:\n",
    "   - Retrieves information from memory relevant to the user's question\n",
    "   - Provides this context to the AI\n",
    "   - Uses the context to generate an informed answer\n",
    "\n",
    "2. **Self-Critique**: This second prompt evaluates the quality of RAG responses:\n",
    "   - Takes the original question, retrieved context, and generated answer\n",
    "   - Classifies the answer as \"Grounded\", \"Ungrounded\", or \"Unclear\"\n",
    "   - Helps ensure responses are properly using retrieved information\n",
    "\n",
    "This pattern creates more reliable AI responses by:\n",
    "- Providing relevant facts from memory\n",
    "- Checking if responses properly use this information\n",
    "- Identifying when responses make claims beyond available information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import (\n",
    "    AzureCognitiveSearchMemoryStore,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search.azure_ai_search_settings import (\n",
    "    AzureAISearchSettings,\n",
    ")\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.memory import SemanticTextMemory\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"generic\"\n",
    "\n",
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the ACS semantic memory\n",
    "    await memory.save_information(COLLECTION_NAME, id=\"info1\", text=\"My name is Andrea\")\n",
    "    await memory.save_information(\n",
    "        COLLECTION_NAME, id=\"info2\", text=\"I currently work as a tour guide\"\n",
    "    )\n",
    "    await memory.save_information(\n",
    "        COLLECTION_NAME, id=\"info3\", text=\"I've been living in Seattle since 2005\"\n",
    "    )\n",
    "    await memory.save_information(\n",
    "        COLLECTION_NAME,\n",
    "        id=\"info4\",\n",
    "        text=\"I visited France and Italy five times since 2015\",\n",
    "    )\n",
    "    await memory.save_information(\n",
    "        COLLECTION_NAME, id=\"info5\", text=\"My family is from New York\"\n",
    "    )\n",
    "\n",
    "\n",
    "azure_ai_search_settings = AzureAISearchSettings.create(\n",
    "    endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_AI_SEARCH_API_KEY\"),\n",
    ")\n",
    "vector_size = 1536\n",
    "\n",
    "\n",
    "acs_connector = AzureCognitiveSearchMemoryStore(\n",
    "    vector_size=vector_size,\n",
    "    search_endpoint=azure_ai_search_settings.endpoint,\n",
    "    admin_key=azure_ai_search_settings.api_key,\n",
    ")\n",
    "\n",
    "memory = SemanticTextMemory(\n",
    "    storage=acs_connector, embeddings_generator=embedding_service\n",
    ")\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "print(\"Populating memory...\")\n",
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"It can give explicit instructions or say 'I don't know' if it does not have an answer.\"\n",
    "\n",
    "sk_prompt_rag = \"\"\"\n",
    "Assistant can have a conversation with you about any topic.\n",
    "\n",
    "Here is some background information about the user that you should use to answer the question below:\n",
    "{{ recall $user_input }}\n",
    "User: {{$user_input}}\n",
    "Assistant: \"\"\".strip()\n",
    "\n",
    "user_input = \"Do I live in Seattle?\"\n",
    "print(f\"Question: {user_input}\")\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat\")\n",
    "chat_func = kernel.add_function(\n",
    "    function_name=\"rag\",\n",
    "    plugin_name=\"RagPlugin\",\n",
    "    prompt=sk_prompt_rag,\n",
    "    prompt_execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(user_input)\n",
    "\n",
    "answer = await kernel.invoke(\n",
    "    chat_func,\n",
    "    user_input=user_input,\n",
    "    chat_history=chat_history,\n",
    ")\n",
    "chat_history.add_assistant_message(str(answer))\n",
    "print(f\"Answer: {str(answer).strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt_rag_sc = \"\"\"\n",
    "You will get a question, background information to be used with that question and a answer that was given.\n",
    "You have to answer Grounded or Ungrounded or Unclear.\n",
    "Grounded if the answer is based on the background information and clearly answers the question.\n",
    "Ungrounded if the answer could be true but is not based on the background information.\n",
    "Unclear if the answer does not answer the question at all.\n",
    "Question: {{$rag_output}}\n",
    "Background: {{ recall $rag_output }}\n",
    "Answer: {{ $input }}\n",
    "Remember, just answer Grounded or Ungrounded or Unclear: \"\"\".strip()\n",
    "\n",
    "\n",
    "self_critique_func = kernel.add_function(\n",
    "    function_name=\"self_critique_rag\",\n",
    "    plugin_name=\"RagPlugin\",\n",
    "    prompt=sk_prompt_rag_sc,\n",
    "    prompt_execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(\n",
    "    self_critique_func, rag_output=answer, input=answer, chat_history=chat_history\n",
    ")\n",
    "print(f\"The answer was {str(check).strip()}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"   Let's pretend the answer was wrong...\")\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(\n",
    "    self_critique_func,\n",
    "    input=answer,\n",
    "    rag_output=\"Yes, you live in New York City.\",\n",
    "    chat_history=chat_history,\n",
    ")\n",
    "print(f\"The answer was {str(check).strip()}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"   Let's pretend the answer is not related...\")\n",
    "print(f\"Answer: {str(answer).strip()}\")\n",
    "check = await kernel.invoke(\n",
    "    self_critique_func,\n",
    "    input=\"Yes, the earth is not flat.\",\n",
    "    rag_output=answer,\n",
    "    chat_history=chat_history,\n",
    ")\n",
    "print(f\"The answer was {str(check).strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# we also add the embedding service to the kernel\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a Fact-Checking System\n",
    "\n",
    "Create a system that:\n",
    "1. Retrieves information from memory about a topic\n",
    "2. Generates a response based on the retrieved information\n",
    "3. Evaluates whether the response is factual based on the retrieved information\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "```python\n",
    "# 1. Add some factual information to memory\n",
    "facts_collection = \"facts\"\n",
    "async def add_facts(memory):\n",
    "    await memory.save_information(facts_collection, \"earth\", \"Earth is the third planet from the Sun and orbits at an average distance of 93 million miles.\")\n",
    "    await memory.save_information(facts_collection, \"moon\", \"The Moon is Earth's only natural satellite and orbits at an average distance of 238,855 miles.\")\n",
    "    await memory.save_information(facts_collection, \"mars\", \"Mars is the fourth planet from the Sun and is often called the 'Red Planet' due to its reddish appearance.\")\n",
    "\n",
    "await add_facts(memory)\n",
    "\n",
    "# 2. Create the fact retrieval prompt\n",
    "fact_prompt = \"\"\"\n",
    "You are a scientific information system that provides accurate facts. You are not allowed to make up information or provide opinions.\n",
    "You will be given a question, and you need to provide the most relevant information from your database.\n",
    "\n",
    "Here is information relevant to the question:\n",
    "{{ recall $question collection='facts' }}\n",
    "\n",
    "Question: {{$question}}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# 3. Create the fact checker prompt\n",
    "checker_prompt = \"\"\"\n",
    "You are a fact-checker evaluating if answers are supported by provided information.\n",
    "\n",
    "INFORMATION: {{ recall $question collection='facts' }}\n",
    "QUESTION: {{$question}}\n",
    "ANSWER: {{$answer}}\n",
    "\n",
    "Evaluate if the answer is:\n",
    "- ACCURATE: Fully supported by the information and directly answers the question\n",
    "- PARTIALLY ACCURATE: Some statements are supported but others go beyond the information\n",
    "- INACCURATE: Contains claims contrary to or unsupported by the information\n",
    "\n",
    "Your assessment:\n",
    "\"\"\".strip()\n",
    "\n",
    "# 4. Create the functions\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"chat\")\n",
    "fact_func = kernel.add_function(\n",
    "    function_name=\"get_fact\",\n",
    "    plugin_name=\"FactSystem\",\n",
    "    prompt=fact_prompt,\n",
    "    prompt_execution_settings=req_settings\n",
    ")\n",
    "\n",
    "checker_func = kernel.add_function(\n",
    "    function_name=\"check_fact\",\n",
    "    plugin_name=\"FactSystem\",\n",
    "    prompt=checker_prompt,\n",
    "    prompt_execution_settings=req_settings\n",
    ")\n",
    "\n",
    "# 5. Test the system\n",
    "async def check_fact(question):\n",
    "    # Get the fact\n",
    "    answer = await kernel.invoke(fact_func, question=question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    \n",
    "    # Check the fact\n",
    "    assessment = await kernel.invoke(checker_func, question=question, answer=str(answer))\n",
    "    print(f\"Assessment: {assessment}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test with questions\n",
    "await check_fact(\"What planet is Earth in our solar system?\")\n",
    "await check_fact(\"How far is the Moon from Earth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution goes here\n",
    "\n",
    "# 1. Add some factual information to memory\n",
    "\n",
    "# 2. Create the fact retrieval prompt\n",
    "\n",
    "# 3. Create the fact checker prompt\n",
    "\n",
    "# 4. Create the functions\n",
    "\n",
    "# 5. Test the system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Persistent Memory with Azure AI Search\n",
    "\n",
    "For production applications, you'll want to use a persistent memory store rather than the in-memory `VolatileMemoryStore`. Azure AI Search (formerly Cognitive Search) provides a powerful, scalable vector database for this purpose.\n",
    "\n",
    "This code demonstrates how to:\n",
    "1. Connect to Azure AI Search\n",
    "2. Use it as a persistent memory store\n",
    "3. Add information that will persist beyond the current session\n",
    "\n",
    "Key differences from the in-memory approach:\n",
    "- Information persists across application restarts\n",
    "- Supports much larger datasets (millions of entries)\n",
    "- Provides additional filtering and hybrid search capabilities\n",
    "- Requires valid Azure credentials and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In summary, the **Semantic Kernel Intro** section covers the following key points:\n",
    "- An overview of SK and its importance in bridging AI and application code.\n",
    "- Core components like the Kernel, AI services, plugins, context, planner, and memory.\n",
    "- How functions (semantic and native) are organized into plugins.\n",
    "- The power of automatic function calling for multi-step AI reasoning.\n",
    "- The role of filters in ensuring secure and validated execution.\n",
    "- Detailed memory management for integrating external knowledge into AI workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
